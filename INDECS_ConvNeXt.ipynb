{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNZNvM1OkDAg"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from numpy import loadtxt\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing\n",
        "from  matplotlib import pyplot as plt\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import KFold\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, model_from_json, Model\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.applications import ResNet50, InceptionV3, EfficientNetB1, EfficientNetB1,EfficientNetB2, VGG19, EfficientNetB5, ConvNeXtXLarge,ConvNeXtLarge\n",
        "from tensorflow.keras.layers import Input, Conv2D,Concatenate, LeakyReLU, Reshape, BatchNormalization, Flatten, Dense, Dropout, AveragePooling2D, MaxPool2D, GlobalAveragePooling2D, Input,  MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2S_l5pRVDxd",
        "outputId": "9856ddaf-86cd-4eda-cc76-659cbe4049ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -u '/content/drive/MyDrive/INDECS.zip' -d '/content/Images'\n",
        "base_dir = '/content/INDECS'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXVmZorckFsz",
        "outputId": "17365e94-166e-4817-ae15-1b66fa24c491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/INDECS.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_cloudy = \"/content/Images/INDECS/Cloudy\"\n",
        "base_sunny = \"/content/Images/INDECS/Sunny\"\n",
        "base_night = \"/content/Images/INDECS/Night\""
      ],
      "metadata": {
        "id": "WXKXw6-_kFrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (112, 112))\n",
        "    # img = img / 255.0\n",
        "    img = np.array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "fe4aZOgBkFnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para determinar o rótulo da imagem\n",
        "def get_image_label(filename, labels):\n",
        "    for i, label in enumerate(labels):\n",
        "        if label in filename:\n",
        "            return i\n",
        "    return len(labels)"
      ],
      "metadata": {
        "id": "JQ15czN_lS_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(directory, labels):\n",
        "    images = []\n",
        "    image_paths = []\n",
        "    image_labels = []\n",
        "\n",
        "    for filename in sorted(os.listdir(directory)):  # Sort the filenames\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            image_paths.append(os.path.join(directory, filename))\n",
        "            label = get_image_label(filename, labels)\n",
        "            image_labels.append(label)\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        image = preprocess_image(image_path)\n",
        "        images.append(image)\n",
        "\n",
        "    images = np.vstack(images)\n",
        "    test_labels = np.array(image_labels)\n",
        "\n",
        "    # one hot encode target values\n",
        "    image_labels = to_categorical(image_labels, num_classes=5)\n",
        "    print(image_labels.shape)\n",
        "    # print(labels[5])\n",
        "\n",
        "    # convert from integers to floats\n",
        "    images = images.astype('float32')\n",
        "    print(images.shape)\n",
        "\n",
        "    return images, image_labels, test_labels"
      ],
      "metadata": {
        "id": "aqY1TNhAlTCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_1 = ['rPR', 'rEO', 'rKT', 'rCR', 'rBO']\n",
        "labels_2 = ['rPA', 'rEO', 'rKT', 'rCR', 'rBO']\n",
        "\n",
        "X_train, y_train, test_labels_train = load_data(base_night,labels_1)\n",
        "\n",
        "X_test_cloudy, y_test_cloudy, test_labels_cloudy  = load_data(base_cloudy,labels_1)\n",
        "X_test_sunny, y_test_sunny, test_labels_sunny = load_data(base_sunny,labels_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6tqe2zM-Bwn",
        "outputId": "f786e08c-c092-4c73-a5fd-bbac6ae50f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1092, 5)\n",
            "(1092, 112, 112, 3)\n",
            "(1080, 5)\n",
            "(1080, 112, 112, 3)\n",
            "(1080, 5)\n",
            "(1080, 112, 112, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(input_shape):\n",
        "    base_model = tf.keras.applications.ConvNeXtXLarge(include_top=False, input_shape=input_shape, weights='imagenet')\n",
        "    for layer in base_model.layers[:-20]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    output_layer = Dense(5, activation='softmax')(x)\n",
        "    return Model(inputs=base_model.input, outputs = output_layer)"
      ],
      "metadata": {
        "id": "zGALiRx_lS2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_input = (112, 112, 3)"
      ],
      "metadata": {
        "id": "Jgyxu7ET0NRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the fused model with both inputs and output\n",
        "fused_model = build_classifier(classifier_input)\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "fused_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "6zE8Srn1-BqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fused_model.summary()"
      ],
      "metadata": {
        "id": "jP-EI5kw-Bn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e79338-c368-44df-9c86-8e50c0a48a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " classifier_input (InputLay  [(None, 112, 112, 3)]        0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " detector_input (InputLayer  [(None, 46)]                 0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " model_2 (Functional)        (None, 3, 3, 128)            3487054   ['classifier_input[0][0]']    \n",
            "                                                          08                                      \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 64)                   3008      ['detector_input[0][0]']      \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 1152)                 0         ['model_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 32)                   2080      ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 1184)                 0         ['flatten[0][0]',             \n",
            "                                                                     'dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 28)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 5)                    5925      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 348716421 (1.30 GB)\n",
            "Trainable params: 67921029 (259.10 MB)\n",
            "Non-trainable params: 280795392 (1.05 GB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping para evitar overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "KMmNwZlOfOO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a callback de ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='melhor_modelo_night.h5',  # O caminho onde o modelo será salvo\n",
        "    save_best_only=True,  # Salva apenas o melhor modelo\n",
        "    monitor='val_accuracy',  # Métrica para determinar o \"melhor\"\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "lGA-fy8NtCQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes\n",
        "classes = np.unique(np.argmax(y_train, axis=1))\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=np.argmax(y_train, axis=1))\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "print(\"Pesos das classes:\", class_weights_dict)"
      ],
      "metadata": {
        "id": "eZVFkd6KE3M1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd680a9f-f48f-4030-a662-9946ea03da13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos das classes: {0: 1.011111111111111, 1: 1.3, 2: 1.011111111111111, 3: 0.56875, 4: 2.022222222222222}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class_weights_dict = {0: 6.0, 1: 1.4, 2: 1.0, 3: 0.56875, 4: 2.2}\n",
        "class_weights_dict = {0: 4.5, 1: 1.3, 2: 1.01111111111111, 3: 0.56875, 4: 2.2}\n",
        "\n",
        "history = fused_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=16, epochs=30, verbose=1,\n",
        "    validation_data=(X_test_cloudy, y_test_cloudy),\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "5WlDwwPt4XMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf4e4e8-3b4d-44d7-c7a3-c56edc2ed423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "69/69 [==============================] - ETA: 0s - loss: 1.3100 - accuracy: 0.5137\n",
            "Epoch 1: val_accuracy improved from -inf to 0.68148, saving model to melhor_modelo_night.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r69/69 [==============================] - 86s 898ms/step - loss: 1.3100 - accuracy: 0.5137 - val_loss: 0.7674 - val_accuracy: 0.6815\n",
            "Epoch 2/30\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.8828\n",
            "Epoch 2: val_accuracy improved from 0.68148 to 0.79352, saving model to melhor_modelo_night.h5\n",
            "69/69 [==============================] - 60s 877ms/step - loss: 0.3921 - accuracy: 0.8828 - val_loss: 0.5491 - val_accuracy: 0.7935\n",
            "Epoch 3/30\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9661\n",
            "Epoch 3: val_accuracy improved from 0.79352 to 0.83519, saving model to melhor_modelo_night.h5\n",
            "69/69 [==============================] - 71s 1s/step - loss: 0.1726 - accuracy: 0.9661 - val_loss: 0.4689 - val_accuracy: 0.8352\n",
            "Epoch 4/30\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9899"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Após o treinamento inicial, descongelar as últimas camadas e continuar o fine-tuning\n",
        "for layer in fused_model.layers[-20:]:\n",
        "    if hasattr(layer, 'trainable'):\n",
        "        layer.trainable = True"
      ],
      "metadata": {
        "id": "-8nnIfUWfeHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-compilar o modelo com uma taxa de aprendizado muito baixa para o fine-tuning\n",
        "fused_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continuar o treinamento para o fine-tuning com taxa de aprendizado reduzida\n",
        "history_fitining = fused_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=16, epochs=30, verbose=1,\n",
        "    validation_data=(X_test_cloudy, y_test_cloudy),\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "dq3dNh3YffYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fused model on the combined test data\n",
        "results_cloudy = fused_model.evaluate(X_test_cloudy, y_test_cloudy)"
      ],
      "metadata": {
        "id": "rfP6BSMSO-Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_sunny = fused_model.evaluate(X_test_sunny, y_test_sunny)"
      ],
      "metadata": {
        "id": "jaBTrPZKO-Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pC = fused_model.predict(X_test_cloudy)\n",
        "predC = np.argmax(pC, axis = 1)\n",
        "pS = fused_model.predict(X_test_sunny)\n",
        "predS = np.argmax(pS, axis = 1)"
      ],
      "metadata": {
        "id": "sten3lnyO-Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predC,test_labels_cloudy))"
      ],
      "metadata": {
        "id": "3YOEbDz8O-HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predS,test_labels_sunny))"
      ],
      "metadata": {
        "id": "JluMeStxPcgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    # Show all ticks and label them with the respective list entries\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "metadata": {
        "id": "cK-NwkTUPcd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(test_labels_cloudy, predC, classes=labels_1, normalize=False, title='Confusion Matrix cloudy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x_hSSDnWPcbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(test_labels_sunny, predS, classes=labels_1, normalize=False, title='Confusion Matrix sunny')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Il1q4tCHPcZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(fused_model, to_file='Model.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "Nu_i4hB7Pjue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the accuracy scores for each training condition\n",
        "scores = {\n",
        "    \"Trained on Cloudy\": {\"Tested on Sunny\": 80.41, \"Tested on Night\": 81.78},\n",
        "    \"Trained on Night\": {\"Tested on Sunny\": 68.17, \"Tested on Cloudy\": 76.18},\n",
        "    \"Trained on Sunny\": {\"Tested on Cloudy\": 79.20, \"Tested on Night\": 82.35}\n",
        "}\n",
        "\n",
        "# Set up the figure and axes for plotting\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 6), sharey=True)\n",
        "fig.suptitle('Model Performance by Training and Testing Conditions')\n",
        "\n",
        "# Colors and labels for the plots\n",
        "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
        "labels = list(scores.keys())\n",
        "\n",
        "# Loop through the scores to plot\n",
        "for ax, (label, tests) in zip(axes, scores.items()):\n",
        "    # Data for the bars\n",
        "    test_labels = list(tests.keys())\n",
        "    accuracies = list(tests.values())\n",
        "\n",
        "    # Create the bar plot\n",
        "    bars = ax.bar(test_labels, accuracies, color=colors[:len(tests)])\n",
        "\n",
        "    # Add the data labels on top of the bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "    ax.set_title(label)\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.set_ylabel('Accuracy (%)')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JiuRsdlZ-rMz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}